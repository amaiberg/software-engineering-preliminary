% 
% This work is licensed under the Creative Commons Attribution-ShareAlike 3.0
% Unported License. To view a copy of this license, visit
% http://creativecommons.org/licenses/by-sa/3.0/.
%
% author: Eric Nyberg
%
\documentclass[oneside,11pt]{memoir}

%\renewcommand{\chaptername}{Task}

\usepackage{times}
\usepackage{setspace}
%\onehalfspacing

\usepackage{titlesec}
\titleformat{\chapter}{\normalfont\Large\bfseries}{Task \thechapter}{1em}{}
\titleformat{\section}{\normalfont\Large\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}{\normalfont\large\bfseries}{\thesubsection}{1em}{}

\usepackage{url}
\usepackage{hyperref}

\usepackage{graphicx}

\usepackage{pstricks}
\usepackage{epsfig}

\usepackage{listings}
\usepackage{color}
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5} 
\definecolor{lightblue}{rgb}{0.95,0.95,1}
\definecolor{mauve}{rgb}{0.58,0,0.82}
\usepackage{multirow}

\definecolor{shadecolor}{gray}{0.9}

\newenvironment{qa}
{\begin{shaded}\begin{itemize}}
{\end{itemize}\end{shaded}}

\title{{\bfseries 11-791 Design and Engineering of Intelligent Information
System \& \\11-693 Software Methods for Biotechnology \\Homework 3}\\
\vspace{1em}
Engineering and Error Analysis with UIMA
\\
{\small This version was built on \today.}
}
\itshape\rmfamily 


\date{}

\begin{document}

\maketitle

\hspace{-0.1\textwidth}
\begin{minipage}{1.2\textwidth}
\vspace{-5em}
\textbf{Important dates}
\begin{itemize}

\item \textbf{Out: October 10.} 

\item \textbf{Due: October 20 (midnight).} 

\end{itemize}

\end{minipage}

\begin{enumerate}

\item \textbf{Submission:} 
Use the same process as for previous assignments (set up GitHub repo, create a Maven 
project, write your code, submit to Maven repo), except that the name has changed to:
hw3-ID

\item \textbf{Write a design report:}
We expect you to document how you designed the execution and
deployment architecture for the sample information system. 
We will extract your documents from your
submitted jar files. 
Remember to include your ID as part of the report file name, and put your name and
ID inside your document. Please submit in PDF format only.
The report should be put in the folder \textbf{src/main/resources/doc}.

%One very special descriptor is the \verb|CpeDescriptor.xml|, which is the entry
%point for the entire pipeline. You are required to name your CPE descriptor
%exactly the same way, and place it in the right place for us to evaluate your
%code.

\item \textbf{Javadocs:} 
Please remember to give an appropriate description (Javadoc comments) for each 
piece of code you create and/or modify.

\item \textbf{Grading:}
This document has two main sections (50pts) each and a bonus section 
for (20pts).
For Section \S~\ref{SectionBuildVS}, you will receive at least 30pts,
if your implementation works correctly: i.e., it produces
the output in the right format and saves
the report file with the right name.
The distribution of 20pts will depend on the quality of your implementation
(i.e., how well your code is documented and written).

For Section \S~\ref{SectionError}, 50pts will be given
to high-quality, creative reports.


\end{enumerate}

\textbf{Useful information}
\begin{enumerate}
\item We encourage you to start Homework 3 as early as possible.

\item 
We expect that most of the general communication between the instructor team and students will take place on 
\href{https://piazza.com/class/hyvsubeilei6dd}{Piazza}.
For private questions, e.g., regarding grades, you may contact instructors by e-mail.
Your friendly TAs are:
\begin{itemize}
\item Avner Maiberg (\href{mailto:amaiberg@andrew.cmu.edu}{\nolinkurl{amaiberg@andrew.cmu.edu}}) 
\item Parag Argawal (\href{mailto:paraga@andrew.cmu.edu}{\nolinkurl{paraga@andrew.cmu.edu}}) 
\item Leonid (Leo) Boytsov (\href{mailto:srchvrs@cmu.edu}{\nolinkurl{srchvrs@cmu.edu}})
\item Xuezi (Manfred) Zhang (\href{mailto:xueziz@andrew.cmu.edu}{\nolinkurl{xueziz@andrew.cmu.edu}}) 
\end{itemize}

\item Source files and pdf file of this assignment are publicly available on 
the  \href{http://github.com/amaiberg/software-engineering-preliminary}{GitHub page}.
Please feel free to fork the project and modify if necessary.
Should you notice any error, feel free to fix it and create/send a pull request (as some
of you did for previous frameworks).  

\end{enumerate}
\hspace{-0.1\textwidth}

\chapter*{Getting started: Creating Maven project from the archetype}
For this task, we have prepared another archetype to help you quickly build your project. 
The tutorial for Homework 1 might help you create a Maven project from an archetype.
For this Homework, you need to add 
\href{https://raw.githubusercontent.com/oaqa/DEIIS-hw3-archetype/master/archetype-catalog.xml}{the new Catalog URL.}


The archetype for Homework is:

\begin{center}\texttt{hw3-archetype}\end{center}

Also remember that the Group Id and Artifact Id for Homework 3 are

\begin{center}
\texttt{edu.cmu.lti.11791.f13.hw3}
\end{center}

and 

\begin{center}\textbf{hw3-ID }\end{center}
with ID being your Andrew Id.
Similarly, you need to edit the pom.xml file to type in the SCM information of
your GitHub repository. You can see that different from other assignment, 
we include a minimal set of descriptors, types, and Java source files.

This set of files defines a complete pipeline. Thus, we may not need
to create additional Java files, descriptors, and or types.
However, you are most welcome to add additional resources if this is necessary for the assignment.

Another important difference is that there is no collection process engine (CPE) descriptor provided.
There is only an aggregate analysis engine (AE) descriptor that imports descriptors of several annotators.
This AE is executed programmatically via a Java file (see section~\ref{SectionAe} for more details):

\begin{center}\textbf{src/main/java/edu/cmu/lti/f14/hw3/hw3-ID/VectorSpaceRetrieval.java} \end{center}

%\newpage
\chapter{Building Vector space Retrieval Model using UIMA (50pts)}
\label{SectionBuildVS}

The purpose of this assignment is to design a simple vector space retrieval system using the UIMA
framework. 
The vector space model is a widely used search engine model that represents query and
document as vector of terms and uses the similarity between these vectors as a model of relevance for
document ranking. 
We will use the cosine similarity measure for this task. 

You will be given set of short documents. Your system should compute the Mean Reciprocal Rank
(MRR) metric for tracking retrieval performance. MRR is useful metric when your system is supposed
to rank the correct document at first position. The mean reciprocal rank is an average of the reciprocal
ranks of results for a sample of queries $Q$:

\begin{equation}\label{EqMRR}
\mbox{MRR} = \sum\limits_{i=1}^{|Q|} \frac{1} {\mbox{rank}_i},
\end{equation}
where $\mbox{rank}_i$ is the rank of the first retrieved relevant document.
Imagine that your retrieval engine retrieves 10 documents:
the three most highly-ranked documents are assessed as not relevant,
but the forth one is considered to be relevant. Then, the reciprocal
rank for this query is equal to $1/4$.

An initial pipeline code and a small text collection is provided to you. 
The code pipeline should work without any compilation errors but it does not produce any relevant results or evaluation measurements.
To run the pipeline from the command line (terminal window), you can type the following:

\begin{center}\small\textbf{mvn exec:java 
       -Dexec.mainClass=edu.cmu.lti.f14.hw1.test3.VectorSpaceRetrieval}\end{center}

We have already given comments inside the code (with //TO DO) so that you will know what is
required. 
You can either fill in those gaps in the existing code, or reorganize the code yourself.
The layout of the provided template code is as follows:

\hspace{-3em}
\begin{minipage}{0.9\textwidth}
\footnotesize
\begin{verbatim}
hw3-ID
|- pom.xml
`- src
     `- main
      |- java/**/*.java        /* Java code */
     |    `src/main/java/edu/cmu/lti/
     |       `f14/hw3/hw3-ID/
     |                   `VectorSpaceRetrieval.java /* In particular:
     |                                                main Java class */
     |                           
     `- resources
       |- data/documents.txt  /* toy data */ 
       |- descriptors/*.xml   /* all pipeline and typesystem descriptors */
       |     `retrievalsystem
       |        `VectorSpaceRetrieval.xml  /* In particular: 
       |                                      the main AAE descriptor */
       `- doc
            `- hw3-ID-report.pdf /* your report */

\end{verbatim}
\normalsize
\end{minipage}



\section{Input}
The text collection for this homework is given in documents.txt file in 
the \textbf{src/main/resource/data} directory of the project. 
For simplicity we have restricted document length to only one sentence. 
We emphasize that each document contains exact one sentence:
There is no need to support multiple sentences in document for this
homework.

Format of the file is described below:

\begin{verbatim}
qid=1 rel=99 John loves Mary 
qid=1 rel=0 John and Mary are friends
qid=1 rel=1 Mary is loved by John
qid=1 rel=0 Mary likes John
qid=2 rel=99 ...
qid=2 rel=1 ...
qid=2 rel=0 ...
\end{verbatim}

Each line has a query id (prefixed by \texttt{qid=}), 
a binary relevance assessment (prefixed by \texttt{rel=}), 
and a document text (which is only one sentence!). 
For documents assessed as relevant, we have \texttt{rel=1},
while \texttt{rel=0} means that a document is not considered to be relevant.

If a line contains a special marker \texttt{rel=99},
the line represents a query rather than a document.
For example, ``qid=1 rel=99 John loves Marry'' means
that there is a query ``John loves Marry'' which has the id one.

\section{Analysis Engine}
\label{SectionAe}
We provide you with a simplified aggregate analysis engine AAE that runs without 
a collection processing engine descriptor (CPE).
This AAE is executed programmatically via the following Java file:

\begin{center}\textbf{src/main/java/edu/cmu/lti/f14/hw3/hw3-ID/VectorSpaceRetrieval.java} \end{center}

The location of this AAE descriptor is:

\begin{center}\textbf{src/main/resources/descriptors/retrievalsystem/VectorSpaceRetrieval.xml} \end{center}

The AAE contains three primitive analysis engines:
\begin{enumerate}
\item DocumentReader
\item DocumentVectorAnnotator
\item RetrievalEvaluator
\end{enumerate}

The DocumentReader parses lines from the input document file and save the parsed information (query id, relevance assessment, and document text) in a CAS.

The DocumentVectorAnnotator is a stub for the annotator that creates sparse term vectors
(recall that a document term vector contains the number of occurrences, i.e., the frequency,
of each word appearing in a document). 
You need to finish an implementation (read the comments/code, and check for //TO DO markers).


The RetrievalEvaluator is a stub for an evaluation component. You need to finish implementation
(again, read the comments/code and check for //TO DO markers).
This evaluation component needs to do the following:
\begin{enumerate}
\item Compute the cosine similarity between the query sentence and each of the subsequent
document sentence; 
\item Using provided relevance assessments, compute a reciprocal rank for each query.
Then, these reciprocal ranks are averaged to obtain the Mean Reciprocal Rank (see Eq.~\ref{EqMRR} for details).
\item Create a report and save it to the file \textbf{report.txt}. 
For each query, your code needs to select a \textbf{relevant} document with the
\textbf{highest} cosine-similarity score and print the following:
\begin{verbatim}
cosine=.. rank=... qid=... rel=1 <document sentence>
\end{verbatim}
In the end of the report, you \textbf{must} print the compute MRR. e.g:
\begin{verbatim}
MRR: 0.334
\end{verbatim}
Your sample report file may look as follows (these scores are for illustrative purposes only 
and may not be true):
\begin{verbatim}
cosine=0.5 rank=1 qid=3 rel=1 The best mirror is an old friend
cosine=0.1 rank=2 qid=4 rel=1 Climate change and energy
MRR: 0.3
\end{verbatim}
We will provide a \textbf{script} to verify the correctness of your output.
\end{enumerate}

\subsection{How to aggregate data coming from different CASes} 
To evaluate performance you may need to aggregate information
supplied with different CASes.
Because the pipeline process one CAS at a time,
this can be problematic unless you persist intermediate results.

UIMA manual discourages us from persisting data \textbf{in the static class members},
because each CAS might be processed by a separate Java virtual machine.
However, you can you use a type system.

Persisting to a file, would work for our assignment, because the pipeline executes runs
the components on the same machine.
In a world of distributed processing, however, you may want to store such information in a database,
network drive, or any other machine-independent storage.


\section{Type System}\label{SectionType}
The type system currently has two types: ``Document'' and ``Token'' 
Feel free to extend this type system if necessary.
At present, the
type Document contains the following information: Relevance Value, Query ID, Text String, Token
List. The Relevance Value, query ID, Text String are extracted from the text collection. 

A list (uima.cas.FSList) of tokens 
represents ``bag-of-words'' feature vectors and have to be constructed for the retrieval system. (\textbf{Reminder}:
one can generate type system classes from the type system XML descriptor via the UIMA JCasGen application)

\textbf{Hint:} tokenList feature of Document needs to be populated by your annotators. 
A Token contains term and its corresponding frequency (in a document). 
You will need to implement the code that populates these sparse frequency vectors.

\chapter{Error Analysis (50pts)}
\label{SectionError}
Analyze and improve your retrieval system's performance by doing error analysis. Write briefly about
why certain cases failed i.e. why the system could not give the highest rank to the correct document
using the cosine similarity measure, what alternative strategy you adopted to resolve this problem, and
why you think your alternative strategy will work. Next you design and implement an alternative
approach.

After testing your new approach,
you need to describe your design, type system and implementation in the report.
The report can be in a free form (you do not have to follow a pre-specified template).
Yet, we expect you to consider the system design from several architectural aspects (UML, type
system, engineering, design pattern, etc.) and several algorithmic aspects (knowledge sources, NLP
tools, machine learning methods, etc.), documenting what you feel is the best report to communicate
the essence of your design. 
For example, it is worth drawing a UML diagram
to represent novel design patterns.
However, please, do not draw diagrams
for well-known design pattern (e.g. Template Method).
Instead, provide a brief textual description.

In summary, your tasks are:
\begin{enumerate}
\item Write a new type system XML descriptor (if necessary),
and generate the type system classes using the UIMA JCasGen application;
\item Correctly extract bag of words feature vector from the input text collection;
\item Compute the cosine similarity between two sentences in the text collection;
\item Compute the Mean Reciprocal Rank (metric) for all sentences in the text collection;
\item Design a better retrieval system that improves the MRR performance measure;
\item Improve the efficiency of the program by doing error analysis of retrieval system.
\end{enumerate}

Finally, do not forget to put
your name and Andrew ID at the top of the document, 
name the file as \textbf{hw3-ID-report.pdf} and put it
under \textbf{src/main/resources/doc}. 

Remember to describe your error analysis in detail, 
as well as to include the relevant comparisons. 
The report is an \textbf{important} part of your submission: 
allocate sufficient time to do a thorough job. 
Feel free to consult the TAs if you have questions regarding the report.

\chapter*{Bonus (20pts)}

Try other similarity measures instead of cosine similarity for ranking (i.e. dice coefficient, Jaccard
coefficient) and include a comparison in your report.


\textbf{Hints:} There is no preferred setup for this assignment, so you may write a new annotator to accomplish this
task, thereby demonstrating your understanding of the UIMA framework and Java programming.

\begin{enumerate}
\item You are allowed to modify the provided type system;
\item You are allowed to modify the provided classes and annotators. You are also allowed to create new
methods, a new class, or even a new package.;
\item If you can think of efficient design patterns for comparing multiple document ranking strategies in a
single execution of the pipeline, feel free to implement!
\end{enumerate}


\end{document}
